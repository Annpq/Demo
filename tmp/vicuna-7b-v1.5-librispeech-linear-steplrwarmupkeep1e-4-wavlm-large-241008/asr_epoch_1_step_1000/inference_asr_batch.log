[2024-10-08 18:57:42,951][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 18:57:42,951][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 18:57:42,951][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 18:57:44,731][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 18:57:50,302][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 18:57:50,303][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 18:57:50,305][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 18:57:50,306][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 18:59:00,133][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 18:59:00,134][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 18:59:00,134][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 18:59:00,225][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 18:59:00,227][slam_llm.utils.train_utils][INFO] - --> asr has 6626.224128 Million params

[2024-10-08 18:59:08,472][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': '/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'train_data_path': None, 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 19:03:35,155][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 19:03:35,155][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 19:03:35,155][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 19:03:36,385][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 19:03:41,914][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:03:41,915][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 19:03:41,917][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:03:41,918][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 19:04:50,798][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 19:04:50,798][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 19:04:50,798][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 19:04:50,882][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 19:04:50,884][slam_llm.utils.train_utils][INFO] - --> asr has 6626.224128 Million params

[2024-10-08 19:04:59,390][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': '/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'train_data_path': None, 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 19:11:43,225][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 19:11:43,225][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 19:11:43,225][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 19:11:44,416][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 19:11:49,889][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:11:49,891][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 19:11:49,893][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:11:49,894][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 19:12:58,082][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 19:12:58,083][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 19:12:58,083][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 19:12:58,167][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 19:12:58,169][slam_llm.utils.train_utils][INFO] - --> asr has 6626.224128 Million params

[2024-10-08 19:13:05,880][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': '/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'train_data_path': None, 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 19:13:59,741][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 19:13:59,741][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 19:13:59,741][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 19:14:01,002][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 19:14:06,530][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:14:06,532][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 19:14:06,534][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:14:06,535][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 19:15:15,775][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 19:15:15,777][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 19:15:15,777][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 19:15:15,874][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 19:15:15,876][slam_llm.utils.train_utils][INFO] - --> asr has 6626.224128 Million params

[2024-10-08 19:15:24,491][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': '/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'train_data_path': None, 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.py', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 19:20:34,949][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 19:20:34,949][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 19:20:34,949][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 19:20:36,206][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 19:20:41,758][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:20:41,759][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 19:20:41,761][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:20:41,762][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 19:21:50,430][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 19:21:50,430][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 19:21:50,430][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 19:21:51,572][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 19:21:57,072][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:21:57,074][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 19:21:57,076][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:21:57,077][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 19:23:05,224][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 19:23:05,225][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 19:23:05,225][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 19:23:05,309][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 19:23:05,311][slam_llm.utils.train_utils][INFO] - --> asr has 6626.224128 Million params

[2024-10-08 19:23:13,554][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 19:24:07,384][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 19:24:07,384][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 19:24:07,385][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 19:24:08,556][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 19:24:13,958][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:24:13,960][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 19:24:13,962][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:24:13,963][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 19:25:21,804][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 19:25:21,804][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 19:25:21,805][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 19:25:21,893][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 19:25:21,895][slam_llm.utils.train_utils][INFO] - --> asr has 6626.224128 Million params

[2024-10-08 19:25:30,793][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 19:26:45,663][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 19:26:45,663][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 19:26:45,663][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 19:26:46,799][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 19:26:52,342][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:26:52,344][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 19:26:52,346][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:26:52,347][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 19:28:01,033][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 19:28:01,034][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 19:28:01,034][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 19:28:01,118][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 19:28:01,121][slam_llm.utils.train_utils][INFO] - --> asr has 6626.224128 Million params

[2024-10-08 19:28:09,025][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 19:36:26,428][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 19:36:26,428][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 19:36:26,428][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 19:36:27,655][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 19:36:33,164][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:36:33,166][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 19:36:33,168][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:36:33,168][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 19:37:41,776][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 19:37:41,776][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 19:37:41,777][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 19:37:41,867][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 19:37:41,869][slam_llm.utils.train_utils][INFO] - --> asr has 6626.224128 Million params

[2024-10-08 19:37:50,478][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 19:37:51,601][root][INFO] - --> Training Set Length = 1
[2024-10-08 19:37:51,602][root][INFO] - =====================================
[2024-10-08 19:44:16,170][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 19:44:16,171][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 19:44:16,171][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 19:44:17,307][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 19:44:22,838][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:44:22,839][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 19:44:22,841][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:44:22,842][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 19:45:31,458][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 19:45:31,461][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 19:45:31,461][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 19:45:31,550][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 19:45:31,552][slam_llm.utils.train_utils][INFO] - --> asr has 6626.224128 Million params

[2024-10-08 19:45:39,741][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 19:45:42,761][root][INFO] - --> Training Set Length = 1
[2024-10-08 19:45:42,761][root][INFO] - =====================================
[2024-10-08 19:46:20,792][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 19:46:20,793][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 19:46:20,793][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 19:46:21,951][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 19:46:27,443][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:46:27,444][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 19:46:27,446][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 19:46:27,447][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 19:47:35,917][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 19:47:35,918][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 19:47:35,919][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 19:47:36,006][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 19:47:36,008][slam_llm.utils.train_utils][INFO] - --> asr has 6626.224128 Million params

[2024-10-08 19:47:45,003][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 19:47:47,637][root][INFO] - --> Training Set Length = 1
[2024-10-08 19:47:47,637][root][INFO] - =====================================
[2024-10-08 20:12:43,126][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 20:12:43,126][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 20:12:43,126][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 20:12:44,271][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 20:12:49,469][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 20:12:49,470][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 20:12:49,472][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 20:12:49,473][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 20:13:56,259][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 20:13:56,263][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 20:13:56,263][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 20:13:56,359][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 20:13:56,362][slam_llm.utils.train_utils][INFO] - --> asr has 6626.224128 Million params

[2024-10-08 20:14:04,305][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 20:14:07,018][root][INFO] - --> Training Set Length = 1
[2024-10-08 20:14:07,018][root][INFO] - =====================================
[2024-10-08 21:34:12,605][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 21:34:12,605][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 21:34:12,605][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 21:34:13,835][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 21:34:19,215][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 21:34:19,217][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 21:34:19,219][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 21:34:19,220][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 21:35:28,049][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 21:35:28,051][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 21:35:28,055][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 21:35:28,057][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 21:35:28,249][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 21:35:28,249][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 21:35:28,250][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 21:35:28,334][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 21:35:28,337][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 21:35:37,205][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 21:35:38,721][root][INFO] - --> Training Set Length = 1
[2024-10-08 21:35:38,721][root][INFO] - =====================================
[2024-10-08 21:51:05,391][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 21:51:05,391][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 21:51:05,391][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 21:51:06,627][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 21:51:11,983][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 21:51:11,985][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 21:51:11,987][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 21:51:11,988][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 21:52:20,727][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 21:52:20,728][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 21:52:20,730][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 21:52:20,731][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 21:52:20,886][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 21:52:20,886][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 21:52:20,887][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 21:52:20,972][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 21:52:20,974][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 21:52:29,341][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 21:52:31,131][root][INFO] - --> Training Set Length = 1
[2024-10-08 21:52:31,131][root][INFO] - =====================================
[2024-10-08 22:11:40,282][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 22:11:40,282][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 22:11:40,282][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 22:11:41,538][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 22:11:46,866][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 22:11:46,868][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 22:11:46,870][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 22:11:46,871][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 22:12:54,821][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 22:12:54,823][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 22:12:54,825][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 22:12:54,827][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 22:12:55,060][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 22:12:55,060][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 22:12:55,060][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 22:12:55,144][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 22:12:55,146][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 22:13:02,960][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 22:13:04,949][root][INFO] - --> Training Set Length = 1
[2024-10-08 22:13:04,949][root][INFO] - =====================================
[2024-10-08 22:18:34,876][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 22:18:34,876][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 22:18:34,876][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 22:18:36,220][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 22:18:41,642][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 22:18:41,644][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 22:18:41,646][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 22:18:41,647][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 22:19:49,997][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 22:19:50,000][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 22:19:50,002][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 22:19:50,003][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 22:19:50,162][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 22:19:50,162][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 22:19:50,162][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 22:19:50,294][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 22:19:50,298][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 22:19:58,406][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 22:20:00,966][root][INFO] - --> Training Set Length = 1
[2024-10-08 22:20:00,967][root][INFO] - =====================================
[2024-10-08 22:22:04,743][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 22:22:04,744][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 22:22:04,744][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 22:22:06,079][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 22:22:11,428][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 22:22:11,430][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 22:22:11,432][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 22:22:11,433][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 22:23:19,639][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 22:23:19,640][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 22:23:19,642][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 22:23:19,642][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 22:23:19,797][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 22:23:19,797][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 22:23:19,797][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 22:23:19,881][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 22:23:19,884][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 22:23:28,163][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 22:23:34,012][root][INFO] - --> Training Set Length = 1
[2024-10-08 22:23:34,012][root][INFO] - =====================================
[2024-10-08 22:25:37,340][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 22:25:37,340][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 22:25:37,340][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 22:25:38,551][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 22:25:43,861][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 22:25:43,863][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 22:25:43,865][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 22:25:43,866][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 22:26:53,659][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 22:26:53,661][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 22:26:53,662][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 22:26:53,663][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 22:26:53,817][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 22:26:53,818][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 22:26:53,818][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 22:26:53,901][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 22:26:53,904][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 22:27:03,103][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 22:47:55,648][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 22:47:55,648][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 22:47:55,648][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 22:47:56,842][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 22:48:02,174][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 22:48:02,176][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 22:48:02,178][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 22:48:02,179][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 22:49:11,307][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 22:49:11,308][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 22:49:11,310][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 22:49:11,311][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 22:49:11,466][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 22:49:11,466][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 22:49:11,466][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 22:49:11,550][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 22:49:11,552][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 22:49:19,917][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 22:53:01,228][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 22:53:01,229][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 22:53:01,229][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 22:53:02,622][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 22:53:08,002][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 22:53:08,004][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 22:53:08,006][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 22:53:08,007][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 22:54:16,261][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 22:54:16,262][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 22:54:16,264][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 22:54:16,264][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 22:54:16,419][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 22:54:16,419][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 22:54:16,420][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 22:54:16,503][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 22:54:16,505][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 22:54:24,948][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 22:54:26,116][root][INFO] - --> Training Set Length = 3
[2024-10-08 22:54:26,116][root][INFO] - =====================================
[2024-10-08 23:02:55,694][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 23:02:55,694][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 23:02:55,694][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 23:02:56,891][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 23:03:02,206][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:03:02,208][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 23:03:02,210][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:03:02,211][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 23:04:10,421][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:04:10,422][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 23:04:10,424][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:04:10,425][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 23:04:10,579][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 23:04:10,579][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 23:04:10,579][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 23:04:10,664][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 23:04:10,666][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 23:04:18,501][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 23:04:20,868][root][INFO] - --> Training Set Length = 3
[2024-10-08 23:04:20,868][root][INFO] - =====================================
[2024-10-08 23:14:02,080][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 23:14:02,081][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 23:14:02,081][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 23:14:03,436][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 23:14:08,758][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:14:08,760][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 23:14:08,762][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:14:08,763][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 23:15:17,517][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:15:17,519][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 23:15:17,522][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:15:17,524][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 23:15:17,711][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 23:15:17,711][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 23:15:17,711][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 23:15:17,795][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 23:15:17,797][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 23:15:28,937][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 23:15:31,588][root][INFO] - --> Training Set Length = 3
[2024-10-08 23:15:31,589][root][INFO] - =====================================
[2024-10-08 23:17:02,624][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 23:17:02,624][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 23:17:02,624][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 23:17:03,966][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 23:17:09,314][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:17:09,316][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 23:17:09,318][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:17:09,319][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 23:18:18,045][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:18:18,049][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 23:18:18,052][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:18:18,054][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 23:18:18,289][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 23:18:18,289][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 23:18:18,289][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 23:18:18,378][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 23:18:18,380][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 23:18:29,410][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 23:18:31,944][root][INFO] - --> Training Set Length = 3
[2024-10-08 23:18:31,944][root][INFO] - =====================================
[2024-10-08 23:20:07,028][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 23:20:07,028][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 23:20:07,028][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 23:20:08,304][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 23:20:13,673][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:20:13,675][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 23:20:13,677][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:20:13,678][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 23:21:21,999][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:21:22,001][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 23:21:22,002][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:21:22,003][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 23:21:22,161][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 23:21:22,161][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 23:21:22,161][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 23:21:22,248][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 23:21:22,250][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 23:21:32,395][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 23:22:17,148][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 23:22:17,148][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 23:22:17,148][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 23:22:18,469][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 23:22:23,804][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:22:23,807][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 23:22:23,809][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:22:23,809][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 23:23:32,438][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:23:32,440][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 23:23:32,442][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:23:32,442][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 23:23:32,608][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 23:23:32,608][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 23:23:32,608][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 23:23:32,694][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 23:23:32,696][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 23:23:43,830][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 23:23:46,459][root][INFO] - --> Training Set Length = 1
[2024-10-08 23:23:46,460][root][INFO] - =====================================
[2024-10-08 23:28:40,427][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 23:28:40,427][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 23:28:40,427][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 23:28:42,958][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 23:28:47,489][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:28:47,491][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 23:28:47,493][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:28:47,493][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 23:29:47,318][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:29:47,321][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 23:29:47,323][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:29:47,325][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 23:29:47,485][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 23:29:47,485][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 23:29:47,486][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 23:29:47,598][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 23:29:47,600][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 23:35:20,692][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 23:35:20,692][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 23:35:20,693][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 23:35:22,012][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 23:35:27,414][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:35:27,416][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 23:35:27,418][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:35:27,419][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 23:36:35,704][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:36:35,705][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 23:36:35,707][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:36:35,708][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 23:36:35,910][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 23:36:35,910][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 23:36:35,910][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 23:36:35,994][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 23:36:35,996][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 23:36:46,419][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 23:36:48,957][root][INFO] - --> Training Set Length = 1
[2024-10-08 23:36:48,957][root][INFO] - =====================================
[2024-10-08 23:38:48,820][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 23:38:48,820][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 23:38:48,820][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 23:38:50,016][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 23:38:55,355][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:38:55,356][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 23:38:55,359][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:38:55,359][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 23:40:03,838][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:40:03,841][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 23:40:03,844][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:40:03,845][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 23:40:04,020][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 23:40:04,020][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 23:40:04,020][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 23:40:04,104][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 23:40:04,106][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 23:40:15,054][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 23:40:17,455][root][INFO] - --> Training Set Length = 1
[2024-10-08 23:40:17,456][root][INFO] - =====================================
[2024-10-08 23:41:00,017][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-08 23:41:00,018][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-08 23:41:00,018][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-08 23:41:01,343][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-08 23:41:06,544][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:41:06,546][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-08 23:41:06,548][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-08 23:41:06,549][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-08 23:42:14,967][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:42:14,970][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-08 23:42:14,974][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-08 23:42:14,976][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-08 23:42:15,174][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-08 23:42:15,174][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-08 23:42:15,174][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-08 23:42:15,260][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-08 23:42:15,262][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-08 23:42:26,235][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-08 23:42:28,127][root][INFO] - --> Training Set Length = 3
[2024-10-08 23:42:28,127][root][INFO] - =====================================
[2024-10-09 17:11:52,541][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-09 17:11:52,541][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-09 17:11:52,541][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-09 17:11:55,174][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-09 17:12:03,994][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-09 17:12:03,997][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-09 17:12:03,999][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-09 17:12:04,000][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-09 17:13:21,308][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-09 17:13:21,309][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-09 17:13:21,311][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-09 17:13:21,312][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-09 17:13:21,455][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-09 17:13:21,455][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-09 17:13:21,455][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-09 17:13:21,576][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-09 17:13:21,578][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-09 17:13:41,048][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-09 17:13:44,113][root][INFO] - --> Training Set Length = 1
[2024-10-09 17:13:44,114][root][INFO] - =====================================
[2024-10-09 17:14:58,393][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-09 17:14:58,393][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-09 17:14:58,393][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-09 17:14:59,881][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-09 17:15:08,778][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-09 17:15:08,782][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-09 17:15:08,785][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-09 17:15:08,786][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-09 17:16:20,198][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-09 17:16:20,199][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-09 17:16:20,201][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-09 17:16:20,202][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-09 17:16:20,349][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-09 17:16:20,350][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-09 17:16:20,350][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-09 17:16:20,431][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-09 17:16:20,433][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-09 17:16:38,011][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-09 17:16:40,111][root][INFO] - --> Training Set Length = 1
[2024-10-09 17:16:40,112][root][INFO] - =====================================
[2024-10-09 17:20:42,425][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-09 17:20:42,425][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-09 17:20:42,426][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-09 17:20:43,944][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-09 17:20:52,858][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-09 17:20:52,860][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-09 17:20:52,862][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-09 17:20:52,863][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-09 17:22:07,868][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-09 17:22:07,873][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-09 17:22:07,879][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-09 17:22:07,881][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-09 17:22:08,106][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-09 17:22:08,106][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-09 17:22:08,107][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-09 17:22:08,217][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-09 17:22:08,220][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-09 17:22:28,039][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-09 17:22:32,256][root][INFO] - --> Training Set Length = 1
[2024-10-09 17:22:32,256][root][INFO] - =====================================
[2024-10-09 19:13:49,741][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-09 19:13:49,741][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-09 19:13:49,742][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-09 19:13:51,000][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-09 19:13:59,738][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-09 19:13:59,740][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-09 19:13:59,743][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-09 19:13:59,743][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-09 19:15:05,566][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-09 19:15:05,571][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-09 19:15:05,577][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-09 19:15:05,579][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-09 19:15:05,797][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-09 19:15:05,797][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-09 19:15:05,797][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-09 19:15:05,866][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-09 19:15:05,869][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-09 19:15:28,242][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-09 19:15:30,619][root][INFO] - --> Training Set Length = 1
[2024-10-09 19:15:30,620][root][INFO] - =====================================
[2024-10-10 18:25:43,091][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-10 18:25:43,092][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-10 18:25:43,092][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-10 18:29:08,834][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'num_workers_dataloader': 2, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 1, 'use_peft': False, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'v_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True}
[2024-10-10 18:29:08,834][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': 'NO_SHARD', 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2024-10-10 18:29:08,834][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'vicuna-7b-v1.5', 'llm_path': 'lmsys/vicuna-7b-v1.5', 'llm_type': 'decoder_only', 'llm_dim': 4096, 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/home/liu.ten/demo/SLAM-LLM/src/slam_llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune'}
[2024-10-10 18:29:10,921][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2024-10-10 18:29:16,282][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-10 18:29:16,283][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2024-10-10 18:29:16,285][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2024-10-10 18:29:16,286][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2024-10-10 18:30:28,244][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-10 18:30:28,246][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 6738.415616 Million params

[2024-10-10 18:30:28,248][slam_llm.utils.train_utils][INFO] - --> Module vicuna-7b-v1.5
[2024-10-10 18:30:28,249][slam_llm.utils.train_utils][INFO] - --> vicuna-7b-v1.5 has 0.0 Million params

[2024-10-10 18:30:28,408][slam_llm.utils.train_utils][INFO] - --> Module linear
[2024-10-10 18:30:28,409][slam_llm.utils.train_utils][INFO] - --> linear has 18.880512 Million params

[2024-10-10 18:30:28,409][slam_model_asr.py][INFO] - loading other parts from: /home/liu.ten/demo/tmp/vicuna-7b-v1.5-librispeech-linear-steplrwarmupkeep1e-4-wavlm-large-241008/asr_epoch_1_step_1000/model.pt
[2024-10-10 18:30:28,470][slam_llm.utils.train_utils][INFO] - --> Model asr
[2024-10-10 18:30:28,472][slam_llm.utils.train_utils][INFO] - --> asr has 18.880512 Million params

[2024-10-10 18:30:40,367][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/train_data.jsonl', 'val_data_path': '/home/liu.ten/demo/SLAM-LLM/examples/asr_librispeech/librispeech_test_clean.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': 'Transcribe speech to text. ', 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2024-10-10 18:30:43,500][root][INFO] - --> Training Set Length = 1
[2024-10-10 18:30:43,500][root][INFO] - =====================================
